{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3edfc8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- System Prompt and JSON Format ---\n",
    "from pydantic import BaseModel, Field, conlist, ValidationError\n",
    "from typing import List, Literal, Optional\n",
    "\n",
    "class VeganStatus(BaseModel):\n",
    "    determination: Literal[\n",
    "        \"Vegan\",\n",
    "        \"Likely Vegan\",\n",
    "        \"Not Vegan\",\n",
    "        \"Contains Non-Vegan Ingredients\",\n",
    "        \"Potentially Non-Vegan\",\n",
    "        \"Undetermined\"\n",
    "    ] = Field(..., description=\"The determined vegan status of the product.\")\n",
    "    reasoning: str = Field(..., description=\"Brief justification for the vegan determination, citing evidence or lack thereof.\")\n",
    "\n",
    "class PrimaryProductAnalysis(BaseModel):\n",
    "    product_name: str = Field(\n",
    "        default=\"N/A\",\n",
    "        description=\"Name of the product as extracted from images, or N/A if unreadable/not identified.\"\n",
    "    )\n",
    "    product_description: str = Field(\n",
    "        default=\"N/A\",\n",
    "        description=\"Factual description from packaging (e.g., 'Instant Coffee'), or N/A if unreadable.\"\n",
    "    )\n",
    "    product_composition: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"List of ingredients extracted from the product packaging.\"\n",
    "    )\n",
    "    allergen_information_explicit: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"List of allergens explicitly stated on the packaging (e.g., in a 'Contains:' section).\"\n",
    "    )\n",
    "    allergen_information_inferred_from_ingredients: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"List of allergens inferred from the product_composition list.\"\n",
    "    )\n",
    "    vegan_status: VeganStatus = Field(\n",
    "        ..., # optional\n",
    "        description=\"Detailed vegan status determination and reasoning.\"\n",
    "    )\n",
    "    other_extracted_text: List[str] = Field(\n",
    "        default_factory=list,\n",
    "        description=\"Any other relevant text snippets from product packaging not fitting elsewhere (e.g., net weight, best before date).\"\n",
    "    )\n",
    "\n",
    "class GeminiProductResponse(BaseModel):\n",
    "    user_audio_transcription: str = Field(\n",
    "        default=\"N/A\",\n",
    "        description=\"Transcription of the user's audio request, or N/A if no/inaudible audio.\"\n",
    "    )\n",
    "    ai_response_to_user: str = Field(\n",
    "        ..., # optional\n",
    "        description=\"The AI's concise, direct, audible-style answer to the user's request or a summary, focusing on the primary product.\"\n",
    "    )\n",
    "    primary_product_analysis: PrimaryProductAnalysis = Field(\n",
    "        ..., # optional\n",
    "        description=\"Detailed analysis of the primary product identified in the images.\"\n",
    "    )\n",
    "\n",
    "PROMPT_FOR_GEMINI_MULTI_IMAGE_AUDIO = \"\"\"\n",
    "You are a highly specialized AI assistant designed to help Visually Impaired Persons (VIPs) understand product information, with a strong focus on allergen and vegan details. You will be provided with:\n",
    "\n",
    "1.  A recorded audio of a user's request (if available). The user is likely holding the product they are asking about.\n",
    "2.  Multiple images of a product (if available). These images will primarily feature the product the user is holding, which should be considered the **primary product**.\n",
    "\n",
    "Your primary goal is to provide clear, concise, and actionable information to the VIP user. DO NOT ask the user to look at the product themselves or refer to visual elements as if they can see them (e.g., avoid \"as you can see\").\n",
    "\n",
    "**Your Tasks:**\n",
    "\n",
    "1.  **Transcribe User Audio:**\n",
    "    *   If audio is provided, accurately transcribe the user's request.\n",
    "    *   If no audio is provided or the audio is inaudible, this field in the JSON should be \"N/A\".\n",
    "\n",
    "2.  **Analyze Primary Product Images:**\n",
    "    *   Focus your analysis on the most prominent product in the images, presumed to be the one the user is holding (the **primary product**).\n",
    "    *   Extract as much textual information as possible from the packaging of this primary product.\n",
    "    *   Identify product name, description, composition (ingredients), explicitly stated allergen information, and any vegan indicators (certifications, logos, explicit statements).\n",
    "\n",
    "3.  **Determine Allergen Information for the Primary Product:**\n",
    "    *   Look for dedicated \"Allergen Information\" sections (e.g., \"Contains: wheat, soy, milk\", \"May contain traces of nuts\"). List these under `allergen_information_explicit`.\n",
    "    *   If no explicit section is found, or to supplement it, carefully review the `product_composition` (ingredients list) for common allergens (e.g., milk, eggs, fish, shellfish, tree nuts, peanuts, wheat, soy, sesame, celery, mustard, lupin, sulphites). List these inferred allergens under `allergen_information_inferred_from_ingredients`. If an ingredient *is* an allergen, list the allergen itself.\n",
    "\n",
    "4.  **Determine Vegan Status for the Primary Product:**\n",
    "    *   **Explicit Indicators:** Prioritize finding vegan certification logos (e.g., \"Certified Vegan\") or explicit textual statements like \"Suitable for Vegans.\"\n",
    "    *   **Ingredient Analysis:** If no explicit indicators, analyze the `product_composition`. Identify common non-vegan ingredients (e.g., meat, poultry, fish, dairy (milk, cheese, whey, casein, lactose), eggs, honey, gelatin, carmine, lanolin, shellac, isinglass). Also note ingredients that are often, but not always, non-vegan and may require further checking if the user had that ability (though you cannot ask them to do this).\n",
    "    *   **Determination Categories for `vegan_status.determination`:**\n",
    "        *   **\"Vegan\"**: If a clear vegan certification/statement is present.\n",
    "        *   **\"Not Vegan\"**: If definitive non-vegan ingredients are listed (e.g., milk, eggs, meat).\n",
    "        *   **\"Contains Non-Vegan Ingredients\"**: Similar to \"Not Vegan,\" use if specific non-vegan items are identified.\n",
    "        *   **\"Likely Vegan\"**: If no explicit certification, but ingredients strongly suggest it (e.g., a product named \"Plant-Based Burger\" with no obvious animal products). State the basis.\n",
    "        *   **\"Potentially Non-Vegan\"**: If ingredients are ambiguous or include items that are often animal-derived but not always (e.g., \"natural flavors,\" some emulsifiers without source specified), and no vegan certification is present. State the basis.\n",
    "        *   **\"Undetermined\"**: If very little information is available to make a call.\n",
    "    *   **Reasoning:** Always provide a brief, clear `reasoning` for the vegan status determination, mentioning key evidence (e.g., \"Vegan certified logo present.\", \"Contains: milk powder, egg yolk.\", \"Ingredient list suggests vegan, but no certification found.\").\n",
    "\n",
    "5.  **Formulate AI Response to User:**\n",
    "    *   This is the most crucial part for the user to hear. It should be a direct, audible-style answer.\n",
    "    *   If there was a `user_audio_transcription` with a specific question, answer that question directly using the information gathered from the `primary_product_analysis`.\n",
    "    *   If the user's request was general (e.g., \"Tell me about this\") or if there was no audio, provide a concise summary of the primary product, prioritizing its name, key allergen information, and vegan status.\n",
    "    *   If information is scarce for the primary product (e.g., blurry image, little text), and you have to make an *inferred assessment* (especially for vegan status or allergens), clearly state the basis for your inference and the level of uncertainty. For example: \"Based on the product name 'Soy Milk' and the partial ingredient 'soybeans' I could identify, it is likely vegan, but I cannot confirm this without a full ingredient list or certification.\" or \"I could only read 'wheat flour' in the ingredients, so it contains wheat. I cannot determine other allergens or its vegan status from the available information.\"\n",
    "    *   If no primary product can be reasonably analyzed from the images, state that you were unable to analyze the product visually.\n",
    "\n",
    "**Output Format:**\n",
    "\n",
    "Return a **single JSON object** strictly following this structure:\n",
    "\n",
    "```json\n",
    "{\n",
    "  \"user_audio_transcription\": \"string (transcription of user's audio, or N/A if no/inaudible audio)\",\n",
    "  \"ai_response_to_user\": \"string (your concise, direct, audible-style answer to the user's request or a summary, focusing on allergens and vegan status of the primary product)\",\n",
    "  \"primary_product_analysis\": {\n",
    "    \"product_name\": \"string (from images, or N/A if no product clearly identified as primary or name unreadable)\",\n",
    "    \"product_description\": \"string (factual description from packaging, e.g., 'Instant Coffee', 'Milk Chocolate Bar', or N/A if unreadable)\",\n",
    "    \"product_composition\": [\n",
    "      \"string (each listed ingredient)\",\n",
    "      ...\n",
    "    ],\n",
    "    \"allergen_information_explicit\": [\n",
    "      \"string (each explicitly stated allergen, e.g., 'Contains wheat, soy.')\",\n",
    "      ...\n",
    "    ],\n",
    "    \"allergen_information_inferred_from_ingredients\": [\n",
    "      \"string (each allergen inferred from the composition list, e.g., 'milk', 'egg')\",\n",
    "      ...\n",
    "    ],\n",
    "    \"vegan_status\": {\n",
    "      \"determination\": \"Vegan\" | \"Likely Vegan\" | \"Not Vegan\" | \"Contains Non-Vegan Ingredients\" | \"Potentially Non-Vegan\" | \"Undetermined\",\n",
    "      \"reasoning\": \"string (brief justification for the vegan determination)\"\n",
    "    },\n",
    "    \"other_extracted_text\": [\n",
    "      \"string (any other relevant text snippets from the product packaging if not fitting elsewhere, e.g., 'Net Wt 100g', 'Best before 12/2025')\",\n",
    "      ...\n",
    "    ]\n",
    "  }\n",
    "}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4197a068",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "playsound is relying on another python subprocess. Please use `pip install pygobject` if you want playsound to run more efficiently.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Initial Configuration ---\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "ALSA lib pcm_direct.c:2048:(snd1_pcm_direct_parse_open_conf) The field ipc_gid must be a valid group (create group audio)\n",
      "ALSA lib pcm_direct.c:2048:(snd1_pcm_direct_parse_open_conf) The field ipc_gid must be a valid group (create group audio)\n",
      "ALSA lib pcm.c:2722:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.rear\n",
      "ALSA lib pcm.c:2722:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.center_lfe\n",
      "ALSA lib pcm.c:2722:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.side\n",
      "ALSA lib pcm_oss.c:404:(_snd_pcm_oss_open) Cannot open device /dev/dsp\n",
      "ALSA lib pcm_oss.c:404:(_snd_pcm_oss_open) Cannot open device /dev/dsp\n",
      "ALSA lib confmisc.c:160:(snd_config_get_card) Invalid field card\n",
      "ALSA lib pcm_usb_stream.c:481:(_snd_pcm_usb_stream_open) Invalid card 'card'\n",
      "ALSA lib confmisc.c:160:(snd_config_get_card) Invalid field card\n",
      "ALSA lib pcm_usb_stream.c:481:(_snd_pcm_usb_stream_open) Invalid card 'card'\n",
      "ALSA lib pcm_direct.c:2048:(snd1_pcm_direct_parse_open_conf) The field ipc_gid must be a valid group (create group audio)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Available audio input devices:\n",
      "  Idx: 4, Name: HD-Audio Generic: ALC245 Analog (hw:1,0), Channels: 2, Rate: 44100\n",
      "  Idx: 6, Name: pulse, Channels: 32, Rate: 44100\n",
      "  Idx: 7, Name: default, Channels: 32, Rate: 44100\n",
      "------------------------------\n",
      "Initializing models and audio...\n",
      "Loading YOLO: detection_model_02.pt\n",
      "  PyTorch CUDA available: True. YOLO will attempt to use: cuda\n",
      "  YOLO model parameters are on device: cpu\n",
      "YOLO model loaded.\n",
      "Gemini API client configured.\n",
      "Loading TTS model: tts_models/en/ljspeech/tacotron2-DDC...\n",
      "  TTS will attempt to use device: cuda\n",
      " > tts_models/en/ljspeech/tacotron2-DDC is already downloaded.\n",
      " > vocoder_models/en/ljspeech/hifigan_v2 is already downloaded.\n",
      " > Using model: Tacotron2\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:True\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:2.718281828459045\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > Model's reduction rate `r` is set to: 1\n",
      " > Vocoder Model: hifigan\n",
      " > Setting up Audio Processor...\n",
      " | > sample_rate:22050\n",
      " | > resample:False\n",
      " | > num_mels:80\n",
      " | > log_func:np.log\n",
      " | > min_level_db:-100\n",
      " | > frame_shift_ms:None\n",
      " | > frame_length_ms:None\n",
      " | > ref_level_db:20\n",
      " | > fft_size:1024\n",
      " | > power:1.5\n",
      " | > preemphasis:0.0\n",
      " | > griffin_lim_iters:60\n",
      " | > signal_norm:False\n",
      " | > symmetric_norm:True\n",
      " | > mel_fmin:0\n",
      " | > mel_fmax:8000.0\n",
      " | > pitch_fmin:1.0\n",
      " | > pitch_fmax:640.0\n",
      " | > spec_gain:1.0\n",
      " | > stft_pad_mode:reflect\n",
      " | > max_norm:4.0\n",
      " | > clip_norm:True\n",
      " | > do_trim_silence:False\n",
      " | > trim_db:60\n",
      " | > do_sound_norm:False\n",
      " | > do_amp_to_db_linear:True\n",
      " | > do_amp_to_db_mel:True\n",
      " | > do_rms_norm:False\n",
      " | > db_level:None\n",
      " | > stats_path:None\n",
      " | > base:2.718281828459045\n",
      " | > hop_length:256\n",
      " | > win_length:1024\n",
      " > Generator Model: hifigan_generator\n",
      " > Discriminator Model: hifigan_discriminator\n",
      "Removing weight norm...\n",
      "TTS model loaded.\n",
      "Initializing Silero VAD and PyAudio...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using cache found in /home/quintuplestuffed/.var/app/com.visualstudio.code/cache/torch/hub/snakers4_silero-vad_master\n",
      "ALSA lib pcm_direct.c:2048:(snd1_pcm_direct_parse_open_conf) The field ipc_gid must be a valid group (create group audio)\n",
      "ALSA lib pcm_direct.c:2048:(snd1_pcm_direct_parse_open_conf) The field ipc_gid must be a valid group (create group audio)\n",
      "ALSA lib pcm.c:2722:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.rear\n",
      "ALSA lib pcm.c:2722:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.center_lfe\n",
      "ALSA lib pcm.c:2722:(snd_pcm_open_noupdate) Unknown PCM cards.pcm.side\n",
      "ALSA lib pcm_oss.c:404:(_snd_pcm_oss_open) Cannot open device /dev/dsp\n",
      "ALSA lib pcm_oss.c:404:(_snd_pcm_oss_open) Cannot open device /dev/dsp\n",
      "ALSA lib confmisc.c:160:(snd_config_get_card) Invalid field card\n",
      "ALSA lib pcm_usb_stream.c:481:(_snd_pcm_usb_stream_open) Invalid card 'card'\n",
      "ALSA lib confmisc.c:160:(snd_config_get_card) Invalid field card\n",
      "ALSA lib pcm_usb_stream.c:481:(_snd_pcm_usb_stream_open) Invalid card 'card'\n",
      "ALSA lib pcm_direct.c:2048:(snd1_pcm_direct_parse_open_conf) The field ipc_gid must be a valid group (create group audio)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "VAD model loaded.\n",
      "Using VAD Device: pulse (Index: 6)\n",
      "Audio stream opened: 48000Hz, 2ch, Format Int16, Buffer 1536 frames.\n",
      "🎤 VAD Monitor Thread Started.\n",
      "Attempting to open camera: /dev/video0\n",
      "Cam using: 3840x2160 @ 30.0 FPS (Codec: YU12)\n",
      "Buffer: JPEG bytes (Max: 240 for 8s), Q:85\n",
      "VAD Active. Speak to trigger processing.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Warning: Ignoring XDG_SESSION_TYPE=wayland on Gnome. Use QT_QPA_PLATFORM=wayland to run on Wayland anyway.\n",
      "Qt: Session management error: Could not open network socket\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Q pressed, exiting...\n",
      "Waiting for VAD thread to join...\n",
      "🎤 VAD Monitor Thread Stopped.\n",
      "Application closed.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import datetime\n",
    "from ultralytics import YOLO\n",
    "from google import genai as google_genai_SDK\n",
    "from google.genai import types as google_genai_types_SDK\n",
    "import typing\n",
    "import numpy as np\n",
    "import json\n",
    "import collections\n",
    "import threading\n",
    "import torch\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import pyaudio\n",
    "from scipy import signal\n",
    "import wave \n",
    "from TTS.api import TTS\n",
    "import playsound \n",
    "\n",
    "# --- Configuration & Constants ---\n",
    "# -- User configurable items (will be prompted or use these as defaults) --\n",
    "VIDEO_DEVICE_DEFAULT = \"/dev/video0\"            # Camera name\n",
    "VAD_SELECTED_DEVICE_INDEX_DEFAULT = 6           # Audio input\n",
    "GEMINI_API_KEY: typing.Optional[str] = None     # Will be ask when run\n",
    "\n",
    "# -- General --\n",
    "TARGET_FPS = 30                                 # Targeted FPS for YOLO, Buffer, etc\n",
    "BUFFER_DURATION_SECONDS = 8                     # Seconds of frame saved as buffer\n",
    "RECORD_DURATION_SECONDS = 6                     # Used frames from buffer to processed\n",
    "FRAME_BUFFER_MAXLEN = int(TARGET_FPS * BUFFER_DURATION_SECONDS)         # Maximum frame count to buffer\n",
    "OUTPUT_BASE_PATH = '/media/gamedisk/!KULIAHH/!Skripsi/github_output'    # Debug photos, audio, JSON path\n",
    "MAX_WORKERS_CPU_BOUND = os.cpu_count() or 4\n",
    "DISPLAY_MAX_WIDTH = 1280\n",
    "DISPLAY_WINDOW_NAME = \"Real-time Vision-Audio Assistant - SPACE: Manual | Q: Quit\"\n",
    "NUM_CLEAREST_CROPS_PER_SECOND_TO_SELECT = 1     # Number of frame per second to be selected as best\n",
    "\n",
    "# -- Camera Settings (set True if using webcam) --\n",
    "WEBCAM_SET_PROPERTIES = False                   # Master switch to attempt setting custom camera properties\n",
    "WEBCAM_DESIRED_WIDTH = 1920                     # Desired camera width\n",
    "WEBCAM_DESIRED_HEIGHT = 1080                    # Desired camera height\n",
    "WEBCAM_DESIRED_FPS = 30                         # Desired camera frame rate\n",
    "WEBCAM_FOURCC_CODEC = 'MJPG'                    # Desired codec (e.g., 'MJPG', 'YUYV')\n",
    "\n",
    "# -- TTS --\n",
    "AI_AUDIO_OUTPUT_ENABLED = True                  # Master switch to Audio Output\n",
    "TTS_MODEL_NAME = \"tts_models/en/ljspeech/tacotron2-DDC\"\n",
    "\n",
    "# -- YOLO --\n",
    "YOLO_MODEL_PATH = 'detection_model_02.pt'\n",
    "YOLO_INFERENCE_SIZE_WIDTH = 640\n",
    "\n",
    "# -- JPEG Buffer --\n",
    "JPEG_QUALITY = 85                               # More bigger, more detailed, bigger file, longer upload, longer time\n",
    "\n",
    "# -- Gemini --\n",
    "GEMINI_MODEL_NAME = 'gemini-2.0-flash'          # Alternative: 'gemini-2.0-flash-lite', 'gemini-2.0-flash', 'gemini-1.5-flash'\n",
    "\n",
    "\n",
    "# -- Silero VAD & Audio Recording --\n",
    "VAD_ENABLED = True                              # Master switch for VAD functionality\n",
    "VAD_DEVICE_CHANNELS = 2                         # Common: 1 or 2. Must match your device.\n",
    "VAD_DEVICE_FORMAT = pyaudio.paInt16             # Common: paInt16, paInt32. Must match.\n",
    "VAD_DEVICE_SAMPLE_RATE_HZ = 48000               # Common: 48000, 44100, 16000. Try device default.\n",
    "\n",
    "SILERO_VAD_TARGET_SAMPLE_RATE = 16000           # Silero VAD expects 16kHz\n",
    "SILERO_VAD_CHUNK_SAMPLES = 512                  # Samples per VAD chunk (32ms @ 16kHz)\n",
    "\n",
    "VAD_SPEECH_CONFIDENCE_THRESHOLD = 0.5           # Threshold for a sound to detected as voice\n",
    "MIN_SPEECH_DURATION_S = 0.1                     # Minimum time of voice to triggered the recording\n",
    "MIN_SILENCE_AFTER_SPEECH_S = 2                  # Minimim time of silent to cut off the recording\n",
    "\n",
    "\n",
    "# --- Global Shared State ---\n",
    "frame_buffer = collections.deque(maxlen=FRAME_BUFFER_MAXLEN)\n",
    "processing_active = False\n",
    "gui_status_message = \"Standby\"\n",
    "last_ai_response_for_display = \"\"\n",
    "shared_data_lock = threading.Lock()\n",
    "\n",
    "yolo_model_global: typing.Optional[YOLO] = None\n",
    "gemini_client_global: typing.Optional[google_genai_SDK.client.Client] = None\n",
    "vad_model_global: typing.Any = None\n",
    "pyaudio_instance: typing.Optional[pyaudio.PyAudio] = None\n",
    "audio_stream: typing.Optional[pyaudio.Stream] = None\n",
    "actual_audio_stream_params: typing.Dict[str, typing.Any] = {}\n",
    "\n",
    "vad_thread_running = True\n",
    "vad_confidence_current = 0.0\n",
    "vad_is_currently_speaking = False\n",
    "vad_is_recording_audio = False\n",
    "vad_audio_chunks_for_processing = []\n",
    "vad_trigger_processing_after_speech = False\n",
    "\n",
    "tts_model_global: typing.Optional[TTS] = None\n",
    "tts_active_lock = threading.Lock()              # Lock to prevent multiple TTS outputs playing at once\n",
    "\n",
    "# --- Helper Functions ---\n",
    "def calculate_sharpness(image: np.ndarray) -> float:\n",
    "    \"\"\"Calculates sharpness of an image using Laplacian variance.\"\"\"\n",
    "    if image.ndim == 3: \n",
    "        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
    "    elif image.ndim == 2: \n",
    "        gray = image\n",
    "    else: \n",
    "        raise ValueError(\"Unsupported image for sharpness.\")\n",
    "    return cv2.Laplacian(gray, cv2.CV_64F).var()\n",
    "\n",
    "def initialize_yolo_model(model_path: str) -> YOLO:\n",
    "    \"\"\"Loads the YOLO model with automatic device selection.\"\"\"\n",
    "    print(f\"Loading YOLO: {model_path}\")\n",
    "    device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "    print(f\"  PyTorch CUDA available: {torch.cuda.is_available()}. YOLO will attempt to use: {device}\")\n",
    "    try:\n",
    "        model = YOLO(model_path)\n",
    "        if next(model.parameters(), None) is not None:\n",
    "            print(f\"  YOLO model parameters are on device: {next(model.parameters()).device}\")\n",
    "        else:\n",
    "            print(f\"  Could not confirm YOLO model device, assuming {device} if PyTorch reports CUDA.\")\n",
    "        print(\"YOLO model loaded.\")\n",
    "        return model\n",
    "    except Exception as e: \n",
    "        print(f\"Error loading YOLO: {e}\"); raise\n",
    "\n",
    "def configure_gemini(api_key_to_use: str) -> google_genai_SDK.client.Client:\n",
    "    \"\"\"Configures and returns the Gemini API client using the provided API key.\"\"\"\n",
    "    if not api_key_to_use:\n",
    "        raise ValueError(\"Gemini API key is not set. Please provide it at script startup.\")\n",
    "    client = google_genai_SDK.Client(api_key=api_key_to_use)\n",
    "    print(\"Gemini API client configured.\"); \n",
    "    return client\n",
    "\n",
    "def setup_output_directories(base_path: str) -> tuple[str, str, str, str, str]:\n",
    "    \"\"\"Creates timestamped run directories for outputs.\"\"\"\n",
    "    ts=datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S\"); run_dir=os.path.join(base_path,ts)\n",
    "    dirs={\"all\":os.path.join(run_dir,\"01_decoded_frames\"),\"audio\":os.path.join(run_dir,\"00_user_audio\"),\"crop\":os.path.join(run_dir,\"02_yolo_crops\"),\"clear\":os.path.join(run_dir,\"03_clear_sel\")}\n",
    "    for d_path in [run_dir] + list(dirs.values()):\n",
    "        if os.path.exists(d_path): \n",
    "            shutil.rmtree(d_path)               # Delete existed directory\n",
    "        os.makedirs(d_path)\n",
    "    print(f\"Created output dirs for run: {run_dir}\"); \n",
    "    return run_dir, dirs[\"all\"], dirs[\"audio\"], dirs[\"crop\"], dirs[\"clear\"]\n",
    "\n",
    "# --- Audio Processing Utilities ---\n",
    "def convert_int_to_float_audio(audio_data_int: np.ndarray, input_format: int) -> np.ndarray:\n",
    "    \"\"\"Converts integer audio data to float32 format.\"\"\"\n",
    "    if input_format == pyaudio.paInt16: \n",
    "        return audio_data_int.astype(np.float32) / 32768.0\n",
    "    elif input_format == pyaudio.paInt32: \n",
    "        return audio_data_int.astype(np.float32) / np.iinfo(np.int32).max\n",
    "    else: raise ValueError(\"Unsupported input format for audio conversion\")\n",
    "\n",
    "def resample_audio_scipy(audio_float: np.ndarray, original_rate: int, target_rate: int) -> np.ndarray:\n",
    "    \"\"\"Resamples float audio data to a target sample rate using SciPy.\"\"\"\n",
    "    if original_rate == target_rate or len(audio_float) == 0: \n",
    "        return audio_float\n",
    "    num_samples_target = int(len(audio_float) * target_rate / original_rate)\n",
    "    if num_samples_target == 0: \n",
    "        return np.array([], dtype=np.float32)\n",
    "    return signal.resample(audio_float, num_samples_target).astype(np.float32)\n",
    "\n",
    "def save_audio_to_wav(audio_frames_list: list, filepath: str, channels: int, sample_rate: int, sample_width_bytes: int) -> bool:\n",
    "    \"\"\"Saves a list of PyAudio byte chunks to a WAV file.\"\"\"\n",
    "    if not audio_frames_list:\n",
    "        print(\"Warning: No audio frames to save.\")\n",
    "        return False\n",
    "    try:\n",
    "        with wave.open(filepath, 'wb') as wf:\n",
    "            wf.setnchannels(channels)\n",
    "            wf.setsampwidth(sample_width_bytes)\n",
    "            wf.setframerate(sample_rate)\n",
    "            wf.writeframes(b''.join(audio_frames_list))\n",
    "        print(f\"Audio saved to {filepath}\")\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error saving WAV file: {e}\")\n",
    "        return False\n",
    "\n",
    "# --- VAD Model and PyAudio Initialization ---\n",
    "def initialize_vad_and_audio_stream(\n",
    "    selected_device_idx: int,\n",
    "    req_channels: int,\n",
    "    req_format: int,\n",
    "    req_sample_rate_hz: typing.Optional[int]\n",
    "    ) -> bool:\n",
    "    \"\"\"Initializes Silero VAD model and PyAudio stream based on selected device.\"\"\"\n",
    "    global vad_model_global, pyaudio_instance, audio_stream, VAD_ENABLED, actual_audio_stream_params\n",
    "    if not VAD_ENABLED:\n",
    "        print(\"VAD is disabled by configuration.\")\n",
    "        return False\n",
    "\n",
    "    print(\"Initializing Silero VAD and PyAudio...\")\n",
    "    try:\n",
    "        torch.set_num_threads(1)\n",
    "        vad_model_global, _ = torch.hub.load(repo_or_dir='snakers4/silero-vad', model='silero_vad', force_reload=False, onnx=False)\n",
    "        print(\"VAD model loaded.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading VAD model: {e}. VAD will be disabled.\")\n",
    "        VAD_ENABLED = False\n",
    "        return False\n",
    "\n",
    "    pyaudio_instance = pyaudio.PyAudio()\n",
    "    \n",
    "    try:\n",
    "        device_info = pyaudio_instance.get_device_info_by_index(selected_device_idx)\n",
    "        print(f\"Using VAD Device: {device_info['name']} (Index: {selected_device_idx})\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error getting device info for index {selected_device_idx}: {e}\"); pyaudio_instance.terminate(); VAD_ENABLED = False; return False\n",
    "\n",
    "    actual_device_sample_rate = req_sample_rate_hz if req_sample_rate_hz else int(device_info['defaultSampleRate'])\n",
    "    \n",
    "    silero_vad_chunk_duration_s = SILERO_VAD_CHUNK_SAMPLES / SILERO_VAD_TARGET_SAMPLE_RATE\n",
    "    pyaudio_frames_per_buffer = int(actual_device_sample_rate * silero_vad_chunk_duration_s)\n",
    "\n",
    "    try:\n",
    "        audio_stream = pyaudio_instance.open(format=req_format,\n",
    "                                       channels=req_channels,\n",
    "                                       rate=actual_device_sample_rate,\n",
    "                                       input=True,\n",
    "                                       input_device_index=selected_device_idx,\n",
    "                                       frames_per_buffer=pyaudio_frames_per_buffer)\n",
    "        print(f\"Audio stream opened: {actual_device_sample_rate}Hz, {req_channels}ch, \"\n",
    "              f\"Format {'Int32' if req_format==pyaudio.paInt32 else 'Int16'}, \"\n",
    "              f\"Buffer {pyaudio_frames_per_buffer} frames.\")\n",
    "        \n",
    "        # Store actual parameters used for the stream to save audio files correctly\n",
    "        actual_audio_stream_params['rate'] = actual_device_sample_rate\n",
    "        actual_audio_stream_params['channels'] = req_channels\n",
    "        actual_audio_stream_params['format'] = req_format\n",
    "        actual_audio_stream_params['width_bytes'] = pyaudio_instance.get_sample_size(req_format)\n",
    "        actual_audio_stream_params['frames_per_buffer'] = pyaudio_frames_per_buffer\n",
    "        \n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f\"Error opening PyAudio stream: {e}. VAD will be disabled.\")\n",
    "        VAD_ENABLED = False\n",
    "        if pyaudio_instance: pyaudio_instance.terminate()\n",
    "        return False\n",
    "\n",
    "# --- VAD Monitor Thread Helpers ---\n",
    "def _process_vad_audio_chunk(raw_bytes: bytes, stream_format: int, stream_channels: int, stream_rate: int, target_rate: int) -> np.ndarray:\n",
    "    \"\"\"Converts raw audio bytes from PyAudio to a resampled mono float array for VAD.\"\"\"\n",
    "    if stream_format == pyaudio.paInt32: \n",
    "        audio_int = np.frombuffer(raw_bytes, dtype=np.int32)\n",
    "    else: \n",
    "        audio_int = np.frombuffer(raw_bytes, dtype=np.int16)\n",
    "\n",
    "    if audio_int.size == 0: \n",
    "        return np.array([], dtype=np.float32)\n",
    "    \n",
    "    audio_mono_int = audio_int[::stream_channels] if stream_channels > 1 else audio_int\n",
    "    audio_mono_float = convert_int_to_float_audio(audio_mono_int, stream_format)\n",
    "    return resample_audio_scipy(audio_mono_float, stream_rate, target_rate)\n",
    "\n",
    "def _prepare_vad_input_tensor(audio_resampled: np.ndarray, silero_chunk_samples: int) -> torch.Tensor:\n",
    "    \"\"\"Pads or truncates resampled audio to the size expected by Silero VAD model.\"\"\"\n",
    "    if audio_resampled.size == 0: \n",
    "        # Return of zeros if input is empty, to avoid error\n",
    "        return torch.zeros(silero_chunk_samples, dtype=torch.float32)\n",
    "\n",
    "    if len(audio_resampled) < silero_chunk_samples:\n",
    "        vad_input_chunk = np.pad(audio_resampled, (0, silero_chunk_samples - len(audio_resampled)), 'constant')\n",
    "    else:\n",
    "        vad_input_chunk = audio_resampled[:silero_chunk_samples]\n",
    "    return torch.from_numpy(vad_input_chunk)\n",
    "\n",
    "# --- VAD Monitor Thread ---\n",
    "def vad_monitor_thread_function():\n",
    "    global vad_thread_running, vad_confidence_current, vad_is_currently_speaking\n",
    "    global vad_is_recording_audio, vad_audio_chunks_for_processing, vad_trigger_processing_after_speech\n",
    "    global shared_data_lock, gui_status_message, processing_active, actual_audio_stream_params\n",
    "\n",
    "    if not VAD_ENABLED or not audio_stream or not vad_model_global:\n",
    "        print(\"VAD thread: VAD not enabled or stream/model not initialized. Exiting.\")\n",
    "        return\n",
    "\n",
    "    print(\"🎤 VAD Monitor Thread Started.\")\n",
    "    speech_start_time, silence_start_time = 0, 0\n",
    "    speech_detected_in_current_segment = False\n",
    "    vad_paused_due_to_processing = False\n",
    "\n",
    "    # Stream parameters used by PyAudio\n",
    "    pyaudio_rate = actual_audio_stream_params.get('rate', VAD_DEVICE_SAMPLE_RATE_HZ)\n",
    "    pyaudio_channels = actual_audio_stream_params.get('channels', VAD_DEVICE_CHANNELS)\n",
    "    pyaudio_format = actual_audio_stream_params.get('format', VAD_DEVICE_FORMAT)\n",
    "    pyaudio_frames_per_chunk = actual_audio_stream_params.get('frames_per_buffer', 512)\n",
    "\n",
    "\n",
    "    while vad_thread_running:\n",
    "        try:\n",
    "            raw_audio_chunk_bytes = audio_stream.read(pyaudio_frames_per_chunk, exception_on_overflow=False)\n",
    "\n",
    "            # Pause VAD logic while the main processing pipeline is running\n",
    "            with shared_data_lock: is_pipeline_processing = processing_active\n",
    "            \n",
    "            if is_pipeline_processing:\n",
    "                if not vad_paused_due_to_processing:\n",
    "                    with shared_data_lock:\n",
    "                        vad_confidence_current = 0.0\n",
    "                        vad_is_currently_speaking = False\n",
    "                vad_paused_due_to_processing = True\n",
    "                speech_start_time, silence_start_time = 0, 0\n",
    "                speech_detected_in_current_segment = False\n",
    "                time.sleep(0.01); continue\n",
    "            \n",
    "            if vad_paused_due_to_processing and not is_pipeline_processing:\n",
    "                vad_paused_due_to_processing = False\n",
    "\n",
    "            # Process audio chunk for VAD\n",
    "            audio_resampled = _process_vad_audio_chunk(raw_audio_chunk_bytes, pyaudio_format, pyaudio_channels, int(pyaudio_rate), SILERO_VAD_TARGET_SAMPLE_RATE)\n",
    "            if audio_resampled.size == 0: continue\n",
    "            \n",
    "            vad_input_tensor = _prepare_vad_input_tensor(audio_resampled, SILERO_VAD_CHUNK_SAMPLES)\n",
    "            current_confidence = vad_model_global(vad_input_tensor, SILERO_VAD_TARGET_SAMPLE_RATE).item()\n",
    "            \n",
    "            with shared_data_lock: \n",
    "                vad_confidence_current = current_confidence\n",
    "                vad_is_currently_speaking = current_confidence > VAD_SPEECH_CONFIDENCE_THRESHOLD\n",
    "\n",
    "            # Determines when to start and stop recording audio, starts after MIN_SPEECH_DURATION_S and stop MIN_SILENCE_AFTER_SPEECH_S\n",
    "            is_speaking_now = current_confidence > VAD_SPEECH_CONFIDENCE_THRESHOLD\n",
    "            if is_speaking_now:\n",
    "                silence_start_time = 0 \n",
    "                if speech_start_time == 0: speech_start_time = time.time()\n",
    "                \n",
    "                if (time.time() - speech_start_time) >= MIN_SPEECH_DURATION_S:\n",
    "                    with shared_data_lock:\n",
    "                        if not vad_is_recording_audio and not processing_active:\n",
    "                            vad_is_recording_audio = True \n",
    "                            vad_audio_chunks_for_processing.clear() \n",
    "                            speech_detected_in_current_segment = True\n",
    "            else: # Silence\n",
    "                if speech_detected_in_current_segment:\n",
    "                    if silence_start_time == 0: silence_start_time = time.time()\n",
    "                    if (time.time() - silence_start_time) >= MIN_SILENCE_AFTER_SPEECH_S:\n",
    "                        with shared_data_lock:\n",
    "                            if vad_is_recording_audio and not processing_active:\n",
    "                                vad_trigger_processing_after_speech = True \n",
    "                                vad_is_recording_audio = False \n",
    "                        speech_detected_in_current_segment = False \n",
    "                        silence_start_time = 0 \n",
    "                speech_start_time = 0 \n",
    "\n",
    "            # Append audio chunks to buffer, if recording is active\n",
    "            with shared_data_lock:\n",
    "                if vad_is_recording_audio:\n",
    "                    vad_audio_chunks_for_processing.append(raw_audio_chunk_bytes)\n",
    "\n",
    "        except IOError as e:\n",
    "            if e.errno == pyaudio.paInputOverflowed: \n",
    "                print(\"VAD Thread: PyAudio Input Overflowed.\")\n",
    "            else: \n",
    "                print(f\"VAD Thread IOError: {e}\")\n",
    "            time.sleep(0.01)\n",
    "        except Exception as e:\n",
    "            print(f\"VAD Thread Error: {e}\")\n",
    "            time.sleep(0.1) \n",
    "\n",
    "    if audio_stream: audio_stream.stop_stream(); audio_stream.close()\n",
    "    if pyaudio_instance: pyaudio_instance.terminate()\n",
    "    print(\"🎤 VAD Monitor Thread Stopped.\")\n",
    "\n",
    "# --- Audio Output Functions ---\n",
    "def initialize_tts_model(model_name: str) -> typing.Optional[TTS]:\n",
    "    \"\"\"Loads the Coqui TTS model.\"\"\"\n",
    "    global AI_AUDIO_OUTPUT_ENABLED\n",
    "    \n",
    "    if not AI_AUDIO_OUTPUT_ENABLED:\n",
    "        print(\"AI audio output is disabled by configuration.\")\n",
    "        return None\n",
    "    try:\n",
    "        print(f\"Loading TTS model: {model_name}...\")\n",
    "        device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "        print(f\"  TTS will attempt to use device: {device}\")\n",
    "        model = TTS(model_name=model_name).to(device)\n",
    "        print(\"TTS model loaded.\")\n",
    "        return model\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading TTS model: {e}. AI audio output will be disabled.\")\n",
    "        AI_AUDIO_OUTPUT_ENABLED = False         # Disable TTS if failed\n",
    "        return None\n",
    "    \n",
    "def generate_and_play_speech_threaded(text_to_speak: str, output_audio_filepath: str):\n",
    "    \"\"\"\n",
    "    Generates speech from text using TTS and plays it. Runs in a separate thread.\n",
    "    Manages a lock to prevent simultaneous TTS operations.\n",
    "    \"\"\"\n",
    "    global gui_status_message, shared_data_lock, tts_model_global\n",
    "\n",
    "    if not tts_model_global or not AI_AUDIO_OUTPUT_ENABLED:\n",
    "        print(\"TTS model not available or AI audio output disabled. Skipping speech.\")\n",
    "        return\n",
    "\n",
    "    # prevents waiting if speech is already playing\n",
    "    if not tts_active_lock.acquire(blocking=False):\n",
    "        print(\"TTS is already active. Skipping new speech request.\")\n",
    "        return\n",
    "\n",
    "    try:\n",
    "        with shared_data_lock:\n",
    "            gui_status_message = \"AI: Generating speech...\"\n",
    "        print(f\"TTS: Generating speech for: \\\"{text_to_speak[:50]}...\\\"\")\n",
    "        \n",
    "        # Generate speech to WAV file\n",
    "        tts_model_global.tts_to_file(\n",
    "            text=text_to_speak,\n",
    "            file_path=output_audio_filepath\n",
    "        )\n",
    "        print(f\"TTS: Speech saved to {output_audio_filepath}\")\n",
    "\n",
    "        with shared_data_lock:\n",
    "            gui_status_message = \"AI: Speaking...\"\n",
    "        \n",
    "        playsound.playsound(output_audio_filepath)\n",
    "\n",
    "        # After playback, reset the GUI status\n",
    "        with shared_data_lock:\n",
    "            if gui_status_message == \"AI: Speaking...\":\n",
    "                gui_status_message = \"Standby (AI spoken)\"\n",
    "\n",
    "    except AttributeError as ae:\n",
    "        print(f\"TTS Error (Attribute): {ae}. Speech generation/playback failed.\")\n",
    "        with shared_data_lock:\n",
    "            gui_status_message = \"AI: Speech Error (Attr)\"\n",
    "    except Exception as e:\n",
    "        print(f\"TTS Error: {e}. Speech generation/playback failed.\")\n",
    "        with shared_data_lock:\n",
    "            gui_status_message = \"AI: Speech output failed.\"\n",
    "    finally:\n",
    "        # Always release the lock so the next request can processed\n",
    "        tts_active_lock.release()\n",
    "\n",
    "# --- Core Processing Logic Functions ---\n",
    "def decode_jpeg_and_save_frame(jpeg_bytes_bboxes_idx_ts_fname):\n",
    "    original_idx, jpeg_bytes, bboxes_for_this_frame, abs_capture_timestamp, frame_storage_path = jpeg_bytes_bboxes_idx_ts_fname\n",
    "    try:\n",
    "        decoded_frame_array = cv2.imdecode(np.frombuffer(jpeg_bytes, np.uint8), cv2.IMREAD_COLOR)\n",
    "        if decoded_frame_array is None: \n",
    "            return None, original_idx, None, None, None\n",
    "        cv2.imwrite(frame_storage_path, decoded_frame_array)\n",
    "        return decoded_frame_array, original_idx, bboxes_for_this_frame, abs_capture_timestamp, frame_storage_path\n",
    "    except Exception as e: \n",
    "        return None, original_idx, None, None, None\n",
    "\n",
    "def save_segment_and_prepare_data_for_cropping(\n",
    "    frames_snapshot_from_buffer: list[tuple[bytes, list, float]], current_trigger_time: float,\n",
    "    segment_duration_sec: int, all_frames_storage_dir: str\n",
    ") -> list[tuple[np.ndarray, list, int, float]]:\n",
    "    segment_start_filter_time = current_trigger_time - segment_duration_sec\n",
    "    selected_segment_data_to_decode = []\n",
    "    temp_idx = 0\n",
    "    for jpeg_bytes, bboxes_list, abs_ts in frames_snapshot_from_buffer:\n",
    "        if abs_ts >= segment_start_filter_time:\n",
    "            frame_filename = os.path.join(all_frames_storage_dir, f\"decoded_frame_{temp_idx:04d}.jpg\")\n",
    "            selected_segment_data_to_decode.append((temp_idx, jpeg_bytes, bboxes_list, abs_ts, frame_filename))\n",
    "            temp_idx += 1\n",
    "    if not selected_segment_data_to_decode: return []\n",
    "    prepared_data_for_cropping = []\n",
    "    decode_start_time = time.perf_counter()\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS_CPU_BOUND) as executor:\n",
    "        futures = {executor.submit(decode_jpeg_and_save_frame, item): item for item in selected_segment_data_to_decode}\n",
    "        for future in as_completed(futures):\n",
    "            decoded_frame, o_idx, bboxes, abs_ts_val, _ = future.result()\n",
    "            if decoded_frame is not None: prepared_data_for_cropping.append((decoded_frame, bboxes, o_idx, abs_ts_val))\n",
    "    print(f\"  JPEG decoding & saving segment took {time.perf_counter() - decode_start_time:.2f}s.\")\n",
    "    if not prepared_data_for_cropping: \n",
    "        return []\n",
    "    prepared_data_for_cropping.sort(key=lambda x: x[2])\n",
    "    actual_segment_start_time = prepared_data_for_cropping[0][3]\n",
    "    final_data = [(f, b, o, ats - actual_segment_start_time) for f,b,o,ats in prepared_data_for_cropping]\n",
    "    print(f\"Decoded frames saved. Prepared {len(final_data)} for cropping.\")\n",
    "    return final_data\n",
    "\n",
    "def crop_products_from_frames(\n",
    "    data_for_cropping: list[tuple[np.ndarray, list, int, float]],\n",
    "    cropped_products_storage_dir: str\n",
    ") -> list[tuple[np.ndarray, int, int, float, str]]:\n",
    "    all_cropped_info = []\n",
    "    if not data_for_cropping: \n",
    "        return []\n",
    "    print(f\"Cropping products from {len(data_for_cropping)} frames...\")\n",
    "    for dec_frame, bboxes_list, o_idx, rel_ts in data_for_cropping:\n",
    "        if not bboxes_list: continue\n",
    "        crop_idx = 0\n",
    "        for (x1,y1,x2,y2,_,_) in bboxes_list:\n",
    "            h,w = dec_frame.shape[:2]; c_x1,c_y1=max(0,x1),max(0,y1); c_x2,c_y2=min(w-1,x2),min(h-1,y2)\n",
    "            if c_x1>=c_x2 or c_y1>=c_y2: continue\n",
    "            crop = dec_frame[c_y1:c_y2, c_x1:c_x2]\n",
    "            if crop.size == 0: continue\n",
    "            fname=f\"dec_f{o_idx:04d}_p{crop_idx:02d}.jpg\"; fp=os.path.join(cropped_products_storage_dir,fname)\n",
    "            cv2.imwrite(fp,crop); all_cropped_info.append((crop.copy(),o_idx,crop_idx,rel_ts,fp)); crop_idx+=1\n",
    "    print(f\"Cropping done. Saved {len(all_cropped_info)} crops.\")\n",
    "    return all_cropped_info\n",
    "\n",
    "def calculate_sharpness_for_item(item_with_idx_data):\n",
    "    item_idx, crop_img_data, orig_idx_val, crop_idx_in_frame_val, rel_ts_val = item_with_idx_data\n",
    "    sharpness_score = calculate_sharpness(crop_img_data)\n",
    "    return item_idx, sharpness_score, crop_img_data, orig_idx_val, crop_idx_in_frame_val, rel_ts_val\n",
    "\n",
    "def select_and_save_clearest_crops(\n",
    "    all_cropped_products_info: list[tuple[np.ndarray, int, int, float, str]],\n",
    "    clearest_selected_storage_dir: str, num_to_select_per_second: int\n",
    ") -> list[str]:\n",
    "    if not all_cropped_products_info: \n",
    "        return []\n",
    "    num_crops = len(all_cropped_products_info)\n",
    "    print(f\"Calculating sharpness for {num_crops} crops in parallel (Max workers: {MAX_WORKERS_CPU_BOUND})...\")\n",
    "    sharpness_task_data = [(i, c[0], c[1], c[2], c[3]) for i, c in enumerate(all_cropped_products_info)]\n",
    "    crops_with_sharpness_temp = [None] * num_crops\n",
    "    sharpness_calc_start_time = time.perf_counter()\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS_CPU_BOUND) as executor:\n",
    "        future_to_idx = {executor.submit(calculate_sharpness_for_item, item_data): item_data[0] for item_data in sharpness_task_data}\n",
    "        for future in as_completed(future_to_idx):\n",
    "            idx, sharp, img, o_idx, c_idx, r_ts = future.result()\n",
    "            crops_with_sharpness_temp[idx] = (sharp, img, o_idx, c_idx, r_ts)\n",
    "    print(f\"  Sharpness calculation took {time.perf_counter() - sharpness_calc_start_time:.2f} seconds.\")\n",
    "    crops_with_sharpness = [item for item in crops_with_sharpness_temp if item is not None]\n",
    "    \n",
    "    # Group detected crops by the second of appearing\n",
    "    crops_by_sec: typing.Dict[int, list] = collections.defaultdict(list)\n",
    "    for sharp, img, o_idx, c_idx, r_ts in crops_with_sharpness: crops_by_sec[int(r_ts // 1.0)].append((sharp, img, o_idx, c_idx))\n",
    "    \n",
    "    selected_paths = []\n",
    "    for sec, crops_in_s in sorted(crops_by_sec.items()):\n",
    "        crops_in_s.sort(key=lambda x: x[0], reverse=True)\n",
    "        # Select the N sharpest crops from this second\n",
    "        for rank, (sharp, img_arr, o_idx, c_idx) in enumerate(crops_in_s[:num_to_select_per_second]):\n",
    "            fname = f\"sel_sec{sec:02d}_rank{rank:02d}_orig{o_idx:04d}_crop{c_idx:02d}.jpg\"\n",
    "            fpath = os.path.join(clearest_selected_storage_dir, fname); cv2.imwrite(fpath, img_arr)\n",
    "            selected_paths.append(fpath)\n",
    "    print(f\"Saved {len(selected_paths)} clearest crops to {clearest_selected_storage_dir}.\")\n",
    "    return selected_paths\n",
    "\n",
    "def _create_gemini_part_from_file(filepath: str, mime_type: str) -> typing.Optional[google_genai_types_SDK.Part]:\n",
    "    \"\"\"Reads a file and creates a Gemini Part object, returns None on error.\"\"\"\n",
    "    try:\n",
    "        if not os.path.exists(filepath):\n",
    "            print(f\"Err: File does not exist: {filepath}. Skip.\")\n",
    "            return None\n",
    "        file_size = os.path.getsize(filepath)\n",
    "        if file_size == 0:\n",
    "            print(f\"Err: File {os.path.basename(filepath)} is 0 bytes. Skip.\")\n",
    "            return None\n",
    "        \n",
    "        with open(filepath, 'rb') as f:\n",
    "            file_bytes = f.read()\n",
    "            if not file_bytes:\n",
    "                print(f\"Err: Read 0 bytes from file {os.path.basename(filepath)}. Skip.\")\n",
    "                return None\n",
    "        return google_genai_types_SDK.Part.from_bytes(data=file_bytes, mime_type=mime_type)\n",
    "    except Exception as e:\n",
    "        print(f\"Err reading/processing file {filepath}. Type: {type(e).__name__}, Error: {e}. Skip.\")\n",
    "        return None\n",
    "\n",
    "def process_images_and_audio_with_gemini( \n",
    "    gemini_client: google_genai_SDK.client.Client,\n",
    "    image_paths: list[str],\n",
    "    audio_filepath: typing.Optional[str],\n",
    "    gemini_model_name: str,\n",
    "    prompt_text: str, \n",
    "    run_output_dir_for_results_json: str\n",
    "):\n",
    "    global last_ai_response_for_display, shared_data_lock\n",
    "    \n",
    "    if not image_paths and not audio_filepath:\n",
    "        print(\"No images or audio to send to Gemini.\")\n",
    "        with shared_data_lock: last_ai_response_for_display = \"Error: No input for Gemini.\"\n",
    "        return\n",
    "\n",
    "    print(f\"\\nPreparing request for Gemini ({gemini_model_name})...\")\n",
    "    contents = [prompt_text] \n",
    "    num_images_to_send = 0\n",
    "    if image_paths:\n",
    "        print(f\"  Processing {len(image_paths)} image paths for Gemini...\")\n",
    "        for img_path in image_paths:\n",
    "            part = _create_gemini_part_from_file(img_path, 'image/jpeg')\n",
    "            if part:\n",
    "                contents.append(part)\n",
    "                num_images_to_send +=1\n",
    "    print(f\"  Added {num_images_to_send} image(s) to Gemini request.\")\n",
    "\n",
    "    processed_audio_filepath = None\n",
    "    if audio_filepath:\n",
    "        part = _create_gemini_part_from_file(audio_filepath, 'audio/wav')\n",
    "        if part:\n",
    "            contents.append(part)\n",
    "            processed_audio_filepath = audio_filepath\n",
    "            print(f\"  Added audio file {os.path.basename(audio_filepath)} to Gemini request.\")\n",
    "        else:\n",
    "            print(f\"Proceeding without audio due to processing error for: {audio_filepath}\")\n",
    "\n",
    "    if num_images_to_send == 0 and not processed_audio_filepath:\n",
    "        print(\"No valid images or audio to send after attempting to load. Aborting Gemini call.\")\n",
    "        with shared_data_lock: last_ai_response_for_display = \"Error: Failed to load inputs.\"\n",
    "        return\n",
    "\n",
    "    gemini_request_config = {\n",
    "        'response_mime_type': 'application/json',\n",
    "        'response_schema': GeminiProductResponse \n",
    "    }\n",
    "\n",
    "    gemini_result_summary = {\n",
    "        \"image_files_sent\": [os.path.basename(p) for p in image_paths if os.path.exists(p) and os.path.getsize(p) > 0][:num_images_to_send] if image_paths else [],\n",
    "        \"audio_file_sent\": os.path.basename(processed_audio_filepath) if processed_audio_filepath else None,\n",
    "        \"gemini_response_raw_text\": None, \"gemini_response_parsed_dict\": None,\n",
    "        \"sdk_parsed_successfully\": False, \"error\": None\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        print(f\"--- Sending request to Gemini ({len(contents)-1} media parts) ---\")\n",
    "        response = gemini_client.models.generate_content(\n",
    "            model=gemini_model_name, contents=contents, config=gemini_request_config\n",
    "        )\n",
    "        gemini_result_summary[\"gemini_response_raw_text\"] = response.text\n",
    "\n",
    "        parsed_response_object = response.parsed\n",
    "        if parsed_response_object and isinstance(parsed_response_object, GeminiProductResponse):\n",
    "            gemini_result_summary[\"gemini_response_parsed_dict\"] = parsed_response_object.model_dump()\n",
    "            gemini_result_summary[\"sdk_parsed_successfully\"] = True\n",
    "            print(\"\\nGemini Output Parsed by SDK (Pydantic object):\\n\", json.dumps(parsed_response_object.model_dump(), indent=2))\n",
    "            with shared_data_lock: last_ai_response_for_display = parsed_response_object.ai_response_to_user\n",
    "        else: \n",
    "            print(f\"Error: Gemini SDK's response.parsed was None or not GeminiProductResponse. Type: {type(parsed_response_object)}\")\n",
    "            with shared_data_lock: last_ai_response_for_display = \"Error: Gemini output can't be parsed.\"\n",
    "\n",
    "    except Exception as e_api:\n",
    "        error_msg = f\"Gemini API call error: {e_api}\"\n",
    "        print(error_msg); gemini_result_summary[\"error\"] = error_msg\n",
    "        with shared_data_lock: last_ai_response_for_display = \"Error: Gemini API call failed.\"\n",
    "    \n",
    "    results_filepath = os.path.join(run_output_dir_for_results_json, \"gemini_processing_results.json\")\n",
    "    with open(results_filepath, 'w') as f_json: json.dump([gemini_result_summary], f_json, indent=2)\n",
    "    print(f\"\\nGemini processing results saved to: {results_filepath}\")\n",
    "\n",
    "# --- Processing Thread Function ---\n",
    "def processing_pipeline_thread_function(\n",
    "    frames_snapshot_to_process: list[tuple[bytes, list, float]],\n",
    "    trigger_time: float,\n",
    "    raw_vad_audio_data: typing.Optional[tuple[list, dict]]\n",
    "):\n",
    "    global processing_active, gui_status_message, gemini_client_global, last_ai_response_for_display\n",
    "\n",
    "    pipeline_start_time = time.perf_counter()\n",
    "    with shared_data_lock: gui_status_message = \"Proc: Setup...\"\n",
    "    run_dir, all_decoded_s_dir, audio_s_dir, cropped_s_dir, clearest_s_dir = setup_output_directories(OUTPUT_BASE_PATH)\n",
    "\n",
    "    permanent_audio_filepath = None\n",
    "    \n",
    "    # Save the captured VAD audio chunks too the save path\n",
    "    if raw_vad_audio_data:\n",
    "        vad_chunks, vad_params = raw_vad_audio_data\n",
    "        if vad_chunks and vad_params:\n",
    "            audio_timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "            user_audio_filename = f\"user_audio_vad_{audio_timestamp}.wav\"\n",
    "            path_to_save_vad_audio = os.path.join(audio_s_dir, user_audio_filename)\n",
    "            \n",
    "            if save_audio_to_wav(vad_chunks, path_to_save_vad_audio,\n",
    "                                 vad_params['channels'], vad_params['rate'], vad_params['width_bytes']):\n",
    "                print(f\"VAD audio saved directly to: {path_to_save_vad_audio}\")\n",
    "                permanent_audio_filepath = path_to_save_vad_audio\n",
    "            else:\n",
    "                print(f\"Failed to save VAD audio to {path_to_save_vad_audio}. Proceeding without VAD audio.\")\n",
    "        else:\n",
    "            print(\"VAD data provided, but chunks or params were empty.\")\n",
    "    \n",
    "    with shared_data_lock: gui_status_message = \"Proc: Decode & Prep Imgs...\"\n",
    "    data_for_cropping = save_segment_and_prepare_data_for_cropping(\n",
    "        frames_snapshot_to_process, trigger_time, RECORD_DURATION_SECONDS, all_decoded_s_dir\n",
    "    )\n",
    "    \n",
    "    clearest_image_paths = []\n",
    "    if data_for_cropping:\n",
    "        with shared_data_lock: gui_status_message = \"Proc: Cropping Imgs...\"\n",
    "        cropped_details = crop_products_from_frames(data_for_cropping, cropped_s_dir)\n",
    "        if cropped_details:\n",
    "            with shared_data_lock: gui_status_message = \"Proc: Calc Sharpness...\"\n",
    "            clearest_image_paths = select_and_save_clearest_crops(cropped_details, clearest_s_dir, NUM_CLEAREST_CROPS_PER_SECOND_TO_SELECT)\n",
    "        else: print(\"No valid product crops from images.\")\n",
    "    else: print(\"No frames decoded from buffer for image processing.\")\n",
    "\n",
    "    if not clearest_image_paths and not permanent_audio_filepath:\n",
    "        print(\"No images and no audio to send to Gemini. Aborting.\")\n",
    "        with shared_data_lock: processing_active = False; gui_status_message = \"Standby (No input for Gemini)\"\n",
    "        return\n",
    "\n",
    "    with shared_data_lock: gui_status_message = \"Proc: Gemini AI...\"\n",
    "    gemini_start_time = time.perf_counter()\n",
    "\n",
    "    process_images_and_audio_with_gemini(\n",
    "        gemini_client_global, clearest_image_paths, permanent_audio_filepath,\n",
    "        GEMINI_MODEL_NAME, PROMPT_FOR_GEMINI_MULTI_IMAGE_AUDIO, run_dir\n",
    "    )\n",
    "    gemini_end_time = time.perf_counter()\n",
    "    print(f\"  Gemini processing took {gemini_end_time - gemini_start_time:.2f} seconds.\")\n",
    "\n",
    "    # after getting a response from Gemini, trigger the TTS output thread\n",
    "    ai_text_for_speech = None\n",
    "    with shared_data_lock:\n",
    "        ai_text_for_speech = last_ai_response_for_display\n",
    "    \n",
    "    if AI_AUDIO_OUTPUT_ENABLED and tts_model_global and ai_text_for_speech and not ai_text_for_speech.startswith(\"Error:\"):\n",
    "        ai_audio_timestamp = datetime.datetime.now().strftime(\"%Y%m%d_%H%M%S_%f\")\n",
    "        ai_speech_filename = f\"ai_response_{ai_audio_timestamp}.wav\"\n",
    "        ai_speech_filepath = os.path.join(audio_s_dir, ai_speech_filename)\n",
    "\n",
    "        print(f\"Preparing to generate AI speech output to: {ai_speech_filepath}\")\n",
    "        tts_thread = threading.Thread(target=generate_and_play_speech_threaded,\n",
    "                                      args=(ai_text_for_speech, ai_speech_filepath))\n",
    "        tts_thread.daemon = True\n",
    "        tts_thread.start()\n",
    "    elif ai_text_for_speech and ai_text_for_speech.startswith(\"Error:\"):\n",
    "        print(\"AI response was an error, not generating speech.\")\n",
    "\n",
    "    run_name = os.path.basename(run_dir)\n",
    "    with shared_data_lock: \n",
    "        processing_active = False \n",
    "        if not tts_active_lock.locked() and not gui_status_message.startswith(\"AI:\"):\n",
    "            pass \n",
    "            \n",
    "    print(f\"Processing pipeline finished for run: {run_name}. Total time: {time.perf_counter() - pipeline_start_time:.2f}s.\")\n",
    "\n",
    "# --- Display Update Function ---\n",
    "def update_display_overlays(\n",
    "    display_frame_to_update: np.ndarray,\n",
    "    current_gui_status: str,\n",
    "    current_ai_response: str,\n",
    "    buffer_current_len: int,\n",
    "    buffer_max_len: int, \n",
    "    current_display_fps: float,\n",
    "    avg_jpeg_encode_time_ms: float,\n",
    "    avg_yolo_infer_time_ms: float,\n",
    "    is_vad_enabled: bool, \n",
    "    current_vad_confidence: float,\n",
    "    is_vad_speaking_raw: bool,\n",
    "    is_main_collecting_audio: bool,\n",
    "    is_pipeline_processing: bool,\n",
    "    ):\n",
    "    \"\"\"Draws all status text and bounding boxes onto the display frame.\"\"\"\n",
    "    y_offset = 30\n",
    "    cv2.putText(display_frame_to_update, current_gui_status, (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2); y_offset += 30\n",
    "    cv2.putText(display_frame_to_update, f\"Buf:{buffer_current_len}/{buffer_max_len}\", (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.7, (0, 255, 0), 2); y_offset += 30\n",
    "    \n",
    "    cv2.putText(display_frame_to_update, f\"DispFPS:{current_display_fps:.1f}\", (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 1); y_offset += 25\n",
    "    cv2.putText(display_frame_to_update, f\"JPEGEnc:{avg_jpeg_encode_time_ms:.1f}ms\", (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 1); y_offset += 25\n",
    "    cv2.putText(display_frame_to_update, f\"YOLOInf:{avg_yolo_infer_time_ms:.1f}ms\", (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 1); y_offset += 25\n",
    "    \n",
    "    if is_vad_enabled:\n",
    "        vad_display_status_str = \"Standby (Processing)\" if is_pipeline_processing else (\"SPEAKING\" if is_vad_speaking_raw else \"SILENT\")\n",
    "        if not is_pipeline_processing and is_main_collecting_audio: vad_display_status_str += \" (REC)\"\n",
    "        cv2.putText(display_frame_to_update, f\"VAD:{current_vad_confidence:.2f} [{vad_display_status_str}]\", (10, y_offset), cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 1); y_offset += 25\n",
    "    \n",
    "    if current_ai_response:\n",
    "        for i, line in enumerate(current_ai_response.split('\\n')):\n",
    "            if y_offset + i*20 > display_frame_to_update.shape[0] - 10 : break\n",
    "            cv2.putText(display_frame_to_update, line, (10, y_offset + i*20), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 200, 255), 1)\n",
    "\n",
    "# --- Main Application Loop ---\n",
    "def main_interactive_loop():\n",
    "    global frame_buffer, processing_active, gui_status_message, last_ai_response_for_display\n",
    "    global yolo_model_global, gemini_client_global, GEMINI_API_KEY\n",
    "    global vad_thread_running, vad_is_recording_audio, vad_audio_chunks_for_processing, vad_trigger_processing_after_speech\n",
    "    global vad_confidence_current, vad_is_currently_speaking, actual_audio_stream_params\n",
    "    global tts_model_global\n",
    "\n",
    "    # --- User Configuration Input ---\n",
    "    print(\"--- Initial Configuration ---\")\n",
    "    GEMINI_API_KEY = input(\"Enter your Gemini API Key: \").strip()\n",
    "    if not GEMINI_API_KEY: print(\"Warning: Gemini API Key not provided. Gemini features will fail.\"); # Or exit\n",
    "\n",
    "    video_device_to_use = input(f\"Enter video device path (e.g., /dev/video0) [{VIDEO_DEVICE_DEFAULT}]: \").strip() or VIDEO_DEVICE_DEFAULT\n",
    "    \n",
    "    selected_audio_device_idx_to_use = VAD_SELECTED_DEVICE_INDEX_DEFAULT\n",
    "    if VAD_ENABLED:\n",
    "        temp_pa = pyaudio.PyAudio()\n",
    "        print(\"\\nAvailable audio input devices:\")\n",
    "        found_devices = False\n",
    "        for i in range(temp_pa.get_device_count()):\n",
    "            dev_info = temp_pa.get_device_info_by_index(i)\n",
    "            if dev_info['maxInputChannels'] > 0:\n",
    "                found_devices = True\n",
    "                print(f\"  Idx: {i}, Name: {dev_info['name']}, Channels: {dev_info['maxInputChannels']}, Rate: {int(dev_info['defaultSampleRate'])}\")\n",
    "        if not found_devices: print(\"No audio input devices found. VAD may not function.\");\n",
    "        else:\n",
    "            try:\n",
    "                choice = input(f\"Enter audio device index for VAD [{VAD_SELECTED_DEVICE_INDEX_DEFAULT}]: \").strip()\n",
    "                selected_audio_device_idx_to_use = int(choice) if choice else VAD_SELECTED_DEVICE_INDEX_DEFAULT\n",
    "            except ValueError:\n",
    "                print(f\"Invalid input, using default device index {VAD_SELECTED_DEVICE_INDEX_DEFAULT}.\")\n",
    "        temp_pa.terminate()\n",
    "        print(\"-\" * 30)\n",
    "\n",
    "    print(\"Initializing models and audio...\")\n",
    "    try:\n",
    "        yolo_model_global = initialize_yolo_model(YOLO_MODEL_PATH)\n",
    "        gemini_client_global = configure_gemini(GEMINI_API_KEY)\n",
    "        tts_model_global = initialize_tts_model(TTS_MODEL_NAME)\n",
    "        \n",
    "        vad_setup_success = False\n",
    "        if VAD_ENABLED:\n",
    "            vad_setup_success = initialize_vad_and_audio_stream(\n",
    "                selected_audio_device_idx_to_use, VAD_DEVICE_CHANNELS, \n",
    "                VAD_DEVICE_FORMAT, VAD_DEVICE_SAMPLE_RATE_HZ\n",
    "            )\n",
    "            if vad_setup_success:\n",
    "                vad_thread = threading.Thread(target=vad_monitor_thread_function, daemon=True)\n",
    "                vad_thread.start()\n",
    "            else: print(\"VAD initialization failed. Will proceed without VAD.\")\n",
    "        else: print(\"VAD is disabled in configuration.\")\n",
    "\n",
    "    except Exception as e: print(f\"Fatal: Initialization error: {e}\"); return\n",
    "\n",
    "    print(f\"Attempting to open camera: {video_device_to_use}\")\n",
    "    cap = cv2.VideoCapture(video_device_to_use)\n",
    "    if not cap.isOpened():\n",
    "        print(f\"Error opening video: {video_device_to_use}.\")\n",
    "        return\n",
    "\n",
    "    # Attempt to set custom camera config\n",
    "    if WEBCAM_SET_PROPERTIES:\n",
    "        print(\"\\n--- Applying Custom Camera Settings ---\")\n",
    "        fourcc = cv2.VideoWriter_fourcc(*WEBCAM_FOURCC_CODEC)\n",
    "        if cap.set(cv2.CAP_PROP_FOURCC, fourcc):\n",
    "            print(f\"  Successfully requested codec: {WEBCAM_FOURCC_CODEC}\")\n",
    "        else:\n",
    "            print(f\"  Warning: Could not set codec to {WEBCAM_FOURCC_CODEC}.\")\n",
    "\n",
    "        cap.set(cv2.CAP_PROP_FRAME_WIDTH, WEBCAM_DESIRED_WIDTH)\n",
    "        cap.set(cv2.CAP_PROP_FRAME_HEIGHT, WEBCAM_DESIRED_HEIGHT)\n",
    "        cap.set(cv2.CAP_PROP_FPS, WEBCAM_DESIRED_FPS)\n",
    "        print(f\"  Requested: {WEBCAM_DESIRED_WIDTH}x{WEBCAM_DESIRED_HEIGHT} @ {WEBCAM_DESIRED_FPS} FPS\")\n",
    "        print(\"---------------------------------------\")\n",
    "    \n",
    "    # Verify and print the actual camera settings being used\n",
    "    cam_w = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
    "    cam_h = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
    "    cam_fps = cap.get(cv2.CAP_PROP_FPS)\n",
    "    cam_codec_int = int(cap.get(cv2.CAP_PROP_FOURCC))\n",
    "    cam_codec_str = \"\".join([chr((cam_codec_int >> 8 * i) & 0xFF) for i in range(4)])\n",
    "\n",
    "    print(f\"Cam using: {cam_w}x{cam_h} @ {cam_fps:.1f} FPS (Codec: {cam_codec_str})\")\n",
    "    if cam_w == 0:\n",
    "        print(\"Cam width is 0. Exiting.\"); cap.release(); return\n",
    "\n",
    "    print(f\"Buffer: JPEG bytes (Max: {FRAME_BUFFER_MAXLEN} for {BUFFER_DURATION_SECONDS}s), Q:{JPEG_QUALITY}\")\n",
    "    if VAD_ENABLED and vad_setup_success: print(\"VAD Active. Speak to trigger processing.\")\n",
    "    else: print(\"VAD Disabled or Failed. Use SPACEBAR to trigger processing.\")\n",
    "\n",
    "    cv2.namedWindow(DISPLAY_WINDOW_NAME, cv2.WINDOW_AUTOSIZE)\n",
    "    last_successful_bboxes = [] \n",
    "    last_display_time = time.time()\n",
    "    yolo_infer_times = collections.deque(maxlen=TARGET_FPS) \n",
    "    jpeg_encode_times = collections.deque(maxlen=TARGET_FPS)\n",
    "    \n",
    "    while True:\n",
    "        ret, raw_frame_4k = cap.read()\n",
    "        if not ret or raw_frame_4k is None: print(\"Error: Can't receive frame. Exiting.\"); break\n",
    "        current_capture_time = time.time()\n",
    "\n",
    "        # --- Real-time YOLO ---\n",
    "        yolo_start_time = time.perf_counter()\n",
    "        orig_h, orig_w = raw_frame_4k.shape[:2]\n",
    "        frame_for_yolo, scale_x, scale_y = raw_frame_4k, 1.0, 1.0 # Default if no resize\n",
    "        if YOLO_INFERENCE_SIZE_WIDTH and orig_w > YOLO_INFERENCE_SIZE_WIDTH:\n",
    "            sfw = YOLO_INFERENCE_SIZE_WIDTH / orig_w; inf_h = int(orig_h*sfw)\n",
    "            frame_for_yolo = cv2.resize(raw_frame_4k, (YOLO_INFERENCE_SIZE_WIDTH, inf_h), cv2.INTER_AREA)\n",
    "            scale_x, scale_y = orig_w/YOLO_INFERENCE_SIZE_WIDTH, orig_h/inf_h\n",
    "        \n",
    "        yolo_results = yolo_model_global.predict(frame_for_yolo, verbose=False)\n",
    "        current_frame_bboxes_for_buffer = []\n",
    "        if yolo_results and yolo_results[0].boxes:\n",
    "            for box in yolo_results[0].boxes:\n",
    "                if yolo_model_global.names[int(box.cls[0])] == \"product\": # Check class name\n",
    "                    x1s,y1s,x2s,y2s=map(int,box.xyxy[0]); cnf=float(box.conf[0])\n",
    "                    ox1,oy1=int(x1s*scale_x),int(y1s*scale_y); ox2,oy2=int(x2s*scale_x),int(y2s*scale_y)\n",
    "                    ox1=max(0,ox1); oy1=max(0,oy1)\n",
    "                    ox2=min(orig_w-1,ox2); oy2=min(orig_h-1,oy2)\n",
    "                    if ox1<ox2 and oy1<oy2: current_frame_bboxes_for_buffer.append((ox1,oy1,ox2,oy2,\"product\",cnf))\n",
    "        last_successful_bboxes = current_frame_bboxes_for_buffer\n",
    "        yolo_infer_times.append((time.perf_counter() - yolo_start_time) * 1000)\n",
    "\n",
    "        # --- JPEG Encode & Buffer ---\n",
    "        jpeg_s_time = time.perf_counter()\n",
    "        s, jpeg_bytes = cv2.imencode('.jpg', raw_frame_4k, [cv2.IMWRITE_JPEG_QUALITY, JPEG_QUALITY])\n",
    "        jpeg_encode_times.append((time.perf_counter() - jpeg_s_time) * 1000)\n",
    "        if s: frame_buffer.append((jpeg_bytes.tobytes(), current_frame_bboxes_for_buffer, current_capture_time))\n",
    "        else: print(\"Warn: JPEG enc failed.\"); continue \n",
    "\n",
    "        # --- Display Logic ---\n",
    "        display_frame = raw_frame_4k.copy()\n",
    "        if display_frame.shape[1] > DISPLAY_MAX_WIDTH: # Resize for display\n",
    "            s_ = DISPLAY_MAX_WIDTH / display_frame.shape[1]; nh_ = int(display_frame.shape[0]*s_)\n",
    "            display_frame = cv2.resize(display_frame, (DISPLAY_MAX_WIDTH, nh_), cv2.INTER_AREA)\n",
    "        \n",
    "        d_h, d_w = display_frame.shape[:2]; d_sx,d_sy = d_w/orig_w, d_h/orig_h\n",
    "        for (x1,y1,x2,y2,_,cnf) in last_successful_bboxes: # Draw bboxes\n",
    "            dx1,dy1=int(x1*d_sx),int(y1*d_sy); dx2,dy2=int(x2*d_sx),int(y2*d_sy)\n",
    "            cv2.rectangle(display_frame,(dx1,dy1),(dx2,dy2),(0,255,0),2)\n",
    "            cv2.putText(display_frame,f\"prod {cnf:.2f}\",(dx1,dy1-5),cv2.FONT_HERSHEY_SIMPLEX,0.5,(0,255,0),1)\n",
    "\n",
    "        l_td = current_capture_time - last_display_time; d_fps = 1.0/l_td if l_td > 0 else 0; last_display_time = current_capture_time\n",
    "        avg_jpeg_ms=sum(jpeg_encode_times)/len(jpeg_encode_times) if jpeg_encode_times else 0\n",
    "        avg_yolo_ms=sum(yolo_infer_times)/len(yolo_infer_times) if yolo_infer_times else 0\n",
    "\n",
    "        with shared_data_lock:\n",
    "            status_disp, ai_resp_disp = gui_status_message, last_ai_response_for_display\n",
    "            vad_conf_disp, vad_speak_disp_raw, vad_rec_main_disp = vad_confidence_current, vad_is_currently_speaking, vad_is_recording_audio\n",
    "            pipeline_active_disp = processing_active\n",
    "        \n",
    "        update_display_overlays(display_frame, status_disp, ai_resp_disp, len(frame_buffer), FRAME_BUFFER_MAXLEN,\n",
    "                                d_fps, avg_jpeg_ms, avg_yolo_ms, \n",
    "                                VAD_ENABLED and vad_setup_success,\n",
    "                                vad_conf_disp, vad_speak_disp_raw, vad_rec_main_disp, pipeline_active_disp)\n",
    "\n",
    "        cv2.imshow(DISPLAY_WINDOW_NAME, display_frame)\n",
    "        key = cv2.waitKey(1) & 0xFF\n",
    "\n",
    "        # --- Trigger Processing ---\n",
    "        should_trigger_now, trigger_source = False, None\n",
    "        if key == ord('q'): print(\"Q pressed, exiting...\"); vad_thread_running = False; break\n",
    "        if key == ord(' '): should_trigger_now, trigger_source = True, \"manual\"\n",
    "        \n",
    "        with shared_data_lock:\n",
    "            if vad_trigger_processing_after_speech:\n",
    "                should_trigger_now, trigger_source = True, \"vad\"\n",
    "                vad_trigger_processing_after_speech = False\n",
    "        \n",
    "        if should_trigger_now:\n",
    "            with shared_data_lock:\n",
    "                if processing_active: print(\"Processing already active. Trigger ignored.\")\n",
    "                else:\n",
    "                    min_frames_needed = int(TARGET_FPS * RECORD_DURATION_SECONDS * 0.5)\n",
    "                    if len(frame_buffer) < min_frames_needed :\n",
    "                        print(f\"Buffering... ({len(frame_buffer)}/{min_frames_needed})\")\n",
    "                        gui_status_message = f\"Buffering... ({len(frame_buffer)})\"\n",
    "                    else:\n",
    "                        processing_active = True\n",
    "                        gui_status_message = f\"Triggered ({trigger_source})! Preparing data...\"\n",
    "                        frames_snapshot, trigger_ts = list(frame_buffer), time.time()\n",
    "                        \n",
    "                        # Package the raw VAD audio chunks to saved\n",
    "                        raw_vad_audio_data_for_pipeline: typing.Optional[tuple[list, dict]] = None\n",
    "                        if trigger_source == \"vad\" and actual_audio_stream_params:\n",
    "                            audio_chunks_to_save = list(vad_audio_chunks_for_processing)\n",
    "                            vad_audio_chunks_for_processing.clear() \n",
    "                            \n",
    "                            if audio_chunks_to_save:\n",
    "                                \n",
    "                                raw_vad_audio_data_for_pipeline = (\n",
    "                                    audio_chunks_to_save, \n",
    "                                    dict(actual_audio_stream_params) # Pass a copy\n",
    "                                )\n",
    "                                print(f\"Collected {len(audio_chunks_to_save)} VAD audio chunks for processing.\")\n",
    "                            else:\n",
    "                                print(\"VAD triggered, but no audio chunks were collected.\")\n",
    "                        \n",
    "                        print(f\"Spawning processing thread (VAD audio chunks: {'Yes' if raw_vad_audio_data_for_pipeline else 'No'})...\")\n",
    "                        thread = threading.Thread(target=processing_pipeline_thread_function, \n",
    "                                                  args=(frames_snapshot, trigger_ts, raw_vad_audio_data_for_pipeline))\n",
    "                        thread.daemon = True; thread.start()\n",
    "    \n",
    "    # Signal all running threads to stop\n",
    "    vad_thread_running = False \n",
    "    if VAD_ENABLED and 'vad_thread' in locals() and vad_thread.is_alive():\n",
    "        print(\"Waiting for VAD thread to join...\"); vad_thread.join(timeout=2.0)\n",
    "    \n",
    "    cap.release(); cv2.destroyAllWindows(); print(\"Application closed.\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    if not os.path.exists(OUTPUT_BASE_PATH):\n",
    "        try: os.makedirs(OUTPUT_BASE_PATH, exist_ok=True)\n",
    "        except OSError as e: print(f\"Error creating output path {OUTPUT_BASE_PATH}: {e}\"); exit(1)\n",
    "    main_interactive_loop()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
